package ai.certifai.mockexam;
import org.datavec.api.records.reader.impl.csv.CSVRecordReader;
import org.datavec.api.split.FileSplit;
import org.datavec.api.transform.TransformProcess;
import org.datavec.api.writable.Writable;
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.deeplearning4j.earlystopping.EarlyStoppingConfiguration;
import org.deeplearning4j.earlystopping.scorecalc.DataSetLossCalculator;
import org.deeplearning4j.earlystopping.termination.MaxEpochsTerminationCondition;
import org.nd4j.common.io.ClassPathResource;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.SplitTestAndTrain;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;
import org.nd4j.linalg.dataset.api.preprocessor.NormalizerMinMaxScaler;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

package ai.certifai.mockexam;
//
//
//
///*
// * Before you start, you should:
// * 1. Put the CSV file into your resources folder
//
// * You are an engineer and have collected a dataset on how different shape of buildings affect the energy efficiency.
// * You have to perform an analysis to determine the heating load and cooling load of the building, given a set
// * of attributes which correspond to a shape. Here, the hints of what you should do is given below:
//
// * Step 1: Read the CSV file in a RecordReader
// * Step 2: Specify the transform process
// * Step 3: Put each instances into the List<List<Writable>>
// * Step 4: Perform shuffling of dataset with seed number provided
// * Step 5: Do train and test splitting
// * Step 6: Do data normalisation in the form of [0,1]
// * Step 7: Define the specifications for early stopping, and subsequently perform early stopping. Your model should not
// *         take more than 200 epochs for training.
// * Step 8: Perform evaluation on both train and test set. Kindly ensure R2 score of outputs on both train and test
// *         is more than +90%.
//
// * Dataset origin: https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#
// * Dataset attribute description:
// * X1: Relative Compactness
// * X2: Surface Area
// * X3: Wall Area
// * X4: Roof Area
// * X5: Overall Height
// * X6: Orientation
// * X7: Glazing Area
// * X8: Glazing Area Distribution
// * Y1: Heating Load
// * Y2: Cooling Load
// */
//
public class EnergyEfficiency {

    //===============Tunable parameters============
    private static int batchSize = 50;
    private static int seed = 123;
    private static double trainFraction = 0.8;
    private static double lr = 0.001;
    //=============================================

    public static void main(String[] args) throws IOException, InterruptedException, IOException {

        /*
         * Step 1: Read the CSV file in a RecordReader
         */
        //====== Your code block starts here===========
        File dataFile = new ClassPathResource("Energy/ENB2012_data.csv").getFile();
        CSVRecordReader CSVreader = new CSVRecordReader(0,',');
        CSVreader.initialize(new FileSplit(dataFile));

        //====== Your code block ends here===========

        Schema inputDataschema = new Schema.Builder()
                .addColumnsDouble("rel-compactness","surface-area","wall-area","roof-area","overall-height")
                .addColumnInteger("orientation")
                .addColumnDouble("glazing-area")
                .addColumnInteger("glazing-area-distribution")
                .addColumnsDouble("heating-load","cooling-load")
                .addColumnsString("emptyCol1","emptyCol2")
                .build();

        /*
         * Step 2: Specify the transform process
         * 2-1: remove unused columns
         * 2-2: filter out empty rows
         */
        //====== Your code block starts here===========
        TransformProcess tp = new TransformProcess.Builder(inputDataSchema)
                .build();

        //====== Your code block ends here===========

        List<List<Writable>> data = new ArrayList<>();

        /*
         * Step 3: Put each instances into the List<List<Writable>>
         * */
        //====== Your code block starts here===========
        List<List<Writable>> originalData = new ArrayList<>();
        while(CSVreader.hasNext()){
            List<Writable> data = CSVreader.next();
            originalData.add(data);
        }


        //====== Your code block ends here===========

        List<List<Writable>> transformed = LocalTransformExecutor.execute(data, tp);
        System.out.println("=======Initial Schema=========\n"+ tp.getInitialSchema());
        System.out.println("=======Final Schema=========\n"+ tp.getFinalSchema());

        /*
         * Check the size of array before and after transformation
         * Please ensure that Pre-transformation size > Post-transformation size
         */
        System.out.println("Size before transform: "+ data.size()+
                "\nColumns before transform: "+ tp.getInitialSchema().numColumns());
        System.out.println("Size after transform: "+ transformed.size()+
                "\nColumns after transform: "+ tp.getFinalSchema().numColumns());

        CollectionRecordReader crr = new CollectionRecordReader(transformed);
        DataSetIterator dataIter = new RecordReaderDataSetIterator(crr,transformedData.size(),13,13, true);

        /*
         * Step 4: Perform shuffling of dataset with seed number provided
         * Do keep in mind that there are 2 targets for this regression problem
         */
        //====== Your code block starts here===========
        DataSet allData = dataIter.next();
        allData.shuffle();

        //====== Your code block ends here===========

        /*
         * Step 5: Do train and test splitting
         */
        //====== Your code block starts here===========
        DataSet allData = dataIter.next();
        allData.shuffle();

        SplitTestAndTrain testTrainSplit = allData.splitTestAndTrain(0.7);

        DataSet trainingSet = testTrainSplit.getTrain();
        DataSet testSet = testTrainSplit.getTest();

        //====== Your code block ends here===========

        /*
         * Step 6: Do data normalisation in the form of [0,1]
         */
        //====== Your code block starts here===========
        DataNormalization normalizer = new NormalizerMinMaxScaler();
        normalizer.fit(trainData);
        normalizer.transform(trainData);
        normalizer.transform(testData);

        //====== Your code block ends here===========

        ViewIterator trainIter = new ViewIterator(trainSet, batchSize);
        ViewIterator testIter = new ViewIterator(testSet, batchSize);

        MultiLayerConfiguration config = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .weightInit(WeightInit.XAVIER)
                .activation(Activation.RELU)
                .updater(new Adam(lr))
                .list()
                .layer(new DenseLayer.Builder()
                        .nIn(trainIter.inputColumns())
                        .nOut(50)
                        .build())
                .layer(new OutputLayer.Builder(LossFunctions.LossFunction.MSE)
                        .nOut(2)
                        .activation(Activation.IDENTITY)
                        .build())
                .build();

        /*
         * Step 7: Perform early stopping so that the model does not overfit
         * Specifications:-
         * a) Calculate average loss on dataset
         * b) terminate if there is no improvement for 1 epoch
         * c) check the loss for each epoch
         * Your model should not take more than 200 epochs for training
         */
        //====== Your code block starts here===========
        EarlyStoppingConfiguration esConfig = new EarlyStoppingConfiguration.Builder()
                .epochTerminationConditions(new MaxEpochsTerminationCondition(epoch))
                .scoreCalculator(new DataSetLossCalculator(testIterator,true))
                .evaluateEveryNEpochs(1)
                .build();

        //====== Your code block ends here===========

        /*
         * Step 8: Perform evaluation for both train and test set using the early stopping model
         */
        //====== Your code block starts here===========
        Evaluation evalTrain = model.evaluate(trainIterator);
        Evaluation evalTest = model.evaluate(testIterator);
        System.out.print("Train Data");
        System.out.println(evalTrain.stats());

        //====== Your code block ends here===========

        System.out.println(evalTrain.stats());
        System.out.println(evalTest.stats());

    }

}
